---
title: "EDA"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(haven) 
```

Import weather data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728", "USW00022534", "USS0023B17S"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2021-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = case_match(
      id, 
      "USW00094728" ~ "CentralPark_NY", 
      "USW00022534" ~ "Molokai_HI",
      "USS0023B17S" ~ "Waterhole_WA"),
    tmin = tmin / 10,
    tmax = tmax / 10,
    month = floor_date(date, unit = "month")) |> 
  select(name, id, everything())
```


Let's make some plots

```{r}
weather_df |> 
  ggplot(aes(x = prcp)) +
  geom_histogram()
```


```{r}
weather_df |> 
  filter(prcp > 1000) #since the initial plot shows outliers, so check if it's a data entry error or real
```


```{r}
weather_df |> 
  filter(tmax > 20, tmax <30) |> 
  ggplot(aes(x = tmin, y = tmax, color = name, shape = name)) +
  geom_point()
#now check the plot to see if there are any outliers
```


## group_by()

```{r}
weather_df |> 
  group_by(name, month)
```


counting stuff

```{r}
weather_df |> 
  group_by(month) |> 
  summarize(
    n_obs = n(), #n() to count
    n_dist = n_distinct(month))
```


```{r}
weather_df |> 
  group_by(name, month) |> 
  summarize(
    n_obs = n())
```


```{r}
weather_df |> 
  count(name) #can also use this specifically for counting OR above codes.
```




## 2x2

```{r}
weather_df |> 
  drop_na(tmax) |> #because without this code, we are finding NA's under cold
  filter(name != "Molokai_HI") |> 
  mutate(
    cold = case_when(
      tmax < 5 ~ "cold",
      tmax >= 5 ~ "not cold"
    )
  ) |> 
  group_by(name, cold) |> 
  summarize(count = n()) #count is number of obs

weather_df |> 
  drop_na(tmax) |> #because without this code, we are finding NA's under cold
  filter(name != "Molokai_HI") |> 
  mutate(
    cold = case_when(
      tmax < 5 ~ "cold",
      tmax >= 5 ~ "not cold"
    )
  ) |> 
  janitor::tabyl(name, cold) #another way to compute a 2x2 table using janitor package
```


## general numeric summaries.

let's try some other useful summaries.

```{r}
weather_df |> 
  group_by(name, month) |> 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE), #na.rm=TRUE to remove missing obs before computing the mean
    median_tmin = median(tmin, na.rm = TRUE),
    sd_prcp = sd(prcp, na.rm = TRUE)
    )
```


  summarize and then plot...

```{r}
weather_df |> 
  group_by(name, month) |> 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE),
    median_tmin = median(tmin, na.rm = TRUE),
    sd_prcp = sd(prcp, na.rm = TRUE)
    ) |> 
  ggplot(aes(x = month, y = mean_tmax, color = name)) +
  geom_point() +
  geom_line()
```


format for readers

```{r}
weather_df |> 
  group_by(name, month) |> 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE)
    ) |> 
  pivot_wider(
    names_from = name,
    values_from = mean_tmax #making data untidy to be presentable 
  ) |> 
  knitr::kable(
    digits = 3, #digits = changing decimal points
    col.names = c("Month", "Central Park", "Molokai", "Waterhole")) 
```



## grouped mutates

```{r}
weather_df |> 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) #mean tmax computes avg tmax across all tmax obs in the dataset and creating a new column called mean_tmax
```

Once you add a grouping layer, tidyverse will remember that grouping.
This can be helpful, but be careful not to forget that you had grouped. 
Use group_by exclusively to do an exploratory analysis, and NOT for data cleaning process(because it will be permanent).

```{r}
weather_df |> 
  group_by(name) |> #if you group_by name first, tmax is different
  mutate(mean_tmax = mean(tmax, na.rm = TRUE))
```

```{r}
weather_df |> 
  group_by(name) |> #if you group_by name first, tmax is different
  mutate(
    mean_tmax = mean(tmax, na.rm = TRUE),
    centered_tmax = tmax - mean_tmax)|> 
  ggplot(aes(x = date, y = centered_tmax, color = name)) +
  geom_point()
```


  Find hottest / coldest days using window function

```{r}
weather_df |> 
  mutate(
    temp_rank = min_rank(tmax) 
  ) |> 
  filter(temp_rank < 4) #top 4 coldest days
```

```{r}
weather_df |> 
  group_by(name) |> 
  mutate(
    temp_rank = min_rank(tmax)
  ) |> 
  filter(temp_rank < 4)
```
  
  
```{r}
weather_df |> 
  group_by(name) |> 
  mutate(
    temp_rank = min_rank(desc(tmax)) #if same value, will rank the same
  ) |> 
  filter(temp_rank < 4)

#can also do below to do the same.

weather_df |> 
  group_by(name) |> 
  filter(min_rank(tmax) < 4) |> 
  arrange(tmax)
```


```{r}
weather_df |> 
  group_by(name) |> 
  mutate(
    lagged_tmax = lag(tmax), #lag_tmax gets the temp the day before the tmax day
    temp_change = tmax - lagged_tmax
  ) |> 
  filter(min_rank(temp_change) < 3) 

#Jan 1 won't have a lagged_temp since its the first day
#BEFORE YOU USE THIS: lag creates a column from the value from the day before, so if your data is not in order, then it will not be smart enough to catch that. 
```


```{r}
weather_df |> 
  group_by(name) |> 
  mutate(
    lagged_tmax = lag(tmax), #lag_tmax gets the temp the day before the tmax day
    temp_change = tmax - lagged_tmax
  ) |> 
  summarize(
    sd_tmax_change = sd(temp_change, na.rm = TRUE)
  ) #can see how much day to day variation do you see
```


## Learning Assessment: 

In the PULSE data, the primary outcome is BDI score; itâ€™s observed over follow-up visits, and we might ask if the typical BDI score values are roughly similar at each. Try to write a code chunk that imports, cleans, and summarizes the PULSE data to examine the mean and median at each visit. Export the results of this in a reader-friendly format.

## PULSE data

```{r}
pulse_df = 
  read_sas("data/public_pulse_data.sas7bdat") |> 
  janitor::clean_names() |>
  pivot_longer(
    bdi_score_bl:bdi_score_12m, 
    names_to = "visit",
    values_to = "bdi_score",
    names_prefix = "bdi_score_"
  )

pulse_df |> 
  group_by(visit) |> 
  summarize(
    mean_bdi = mean(bdi_score, na.rm = TRUE),
    median_bdi = median(bdi_score, na.rm = TRUE)
  ) |> 
  knitr::kable(digits = 1)
```


## FAS

```{r}
litters_df = 
  read_csv("data/FAS_litters.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  separate(
    group, into = c("dose", "tx_day"), sep = 3
  )

pups_df = 
  read_csv("data/FAS_pups.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names()

fas_df = 
  left_join(pups_df, litters_df, by = "litter_number")
```


Compute a table that we care about.

```{r}
fas_df |> 
  drop_na(dose) |> #without the drop code, one row has NA in it, but sake of making it easy for the assessment, can just drop.
  group_by(dose, tx_day) |> 
  summarize(mean_pivot = mean(pd_pivot, na.rm = TRUE)) |> 
  pivot_wider(  #make it into a presentable format
    names_from = tx_day,
    values_from = mean_pivot
  ) |> 
  knitr::kable(digits = 2)
```

